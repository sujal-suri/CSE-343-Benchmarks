{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9157d58-rebrand",
   "metadata": {},
   "source": [
    "# JointMatch Reimagined: Tiny ImageNet SSL (Kaggle Optimized)\n",
    "\n",
    "This notebook implements the JointMatch semi-supervised learning algorithm for image classification on the Tiny ImageNet dataset using a custom PyTorch training loop, optimized for Kaggle.\n",
    "\n",
    "**Core Idea:** Leverage two models teaching each other with adaptive thresholds, without relying on external training frameworks like Ignite.\n",
    "\n",
    "**References:**\n",
    "*   **JointMatch Paper:** Zou, H. P., & Caragea, C. (2023). JointMatch: A Unified Approach for Diverse and Collaborative Pseudo-Labeling to Semi-Supervised Text Classification. *EMNLP 2023*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba436f-rebrand",
   "metadata": {},
   "source": [
    "## 1. Setup & Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd79a9-rebrand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages (EfficientNet and dataset downloader)\n",
    "print(\"Installing dependencies...\")\n",
    "!pip install -q opendatasets efficientnet-pytorch tqdm\n",
    "print(\"Dependencies installed.\")\n",
    "\n",
    "# --- Verification ---\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"Torchvision Version: {torchvision.__version__}\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA Version detected by PyTorch: {torch.version.cuda}\")\n",
    "        print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing torch/torchvision: {e}. Ensure PyTorch is installed.\")\n",
    "except Exception as e:\n",
    "     print(f\"An error occurred during verification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeafafa-rebrand",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d3c06-rebrand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import shutil\n",
    "import warnings\n",
    "import copy\n",
    "from math import floor, ceil\n",
    "from sklearn.model_selection import train_test_split # Still useful for potential splits\n",
    "from itertools import cycle\n",
    "import time\n",
    "import glob\n",
    "from tqdm.notebook import tqdm # Progress bar\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import models, datasets\n",
    "from torchvision import transforms as T\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9608498b-rebrand",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffca161-rebrand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Setup\n",
    "SEED = 42\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Data Parameters ---\n",
    "DATA_DIR_KAGGLE = '/kaggle/input/tiny-imagenet-200/tiny-imagenet-200'\n",
    "DATA_DIR_MANUAL = './tiny-imagenet-200'\n",
    "MANUAL_DOWNLOAD = True # Set True to download, False if using Kaggle dataset input\n",
    "DATA_DIR = DATA_DIR_MANUAL if MANUAL_DOWNLOAD else DATA_DIR_KAGGLE\n",
    "\n",
    "NUM_CLASSES = 200\n",
    "IMG_SIZE = 64\n",
    "VAL_IMAGES_PER_CLASS = 50\n",
    "\n",
    "# --- Semi-Supervised Learning Parameters ---\n",
    "NUM_LABELED_PER_CLASS = 10\n",
    "TOTAL_TRAIN_IMAGES_PER_CLASS = 500\n",
    "\n",
    "# --- JointMatch Specific Hyperparameters ---\n",
    "ema_decay = 0.999\n",
    "base_threshold = 0.95\n",
    "disagreement_weight = 0.7\n",
    "unlabeled_loss_weight = 1.0\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "batch_size = 64\n",
    "unlabeled_ratio = 7\n",
    "labeled_batch_size = batch_size // (unlabeled_ratio + 1)\n",
    "unlabeled_batch_size = batch_size - labeled_batch_size\n",
    "print(f\"Total Batch Size: {batch_size}\")\n",
    "print(f\"  Labeled Batch Size: {labeled_batch_size}\")\n",
    "print(f\"  Unlabeled Batch Size: {unlabeled_batch_size}\")\n",
    "\n",
    "lr = 3e-4\n",
    "num_epochs = 50 # Increased epochs for better convergence potential\n",
    "num_labeled_total = NUM_LABELED_PER_CLASS * NUM_CLASSES\n",
    "# Define steps per epoch based on labeled data\n",
    "steps_per_epoch = ceil(num_labeled_total / labeled_batch_size)\n",
    "print(f\"Steps per epoch (based on labeled data): {steps_per_epoch}\")\n",
    "gradient_accumulation_steps = 1 # Set > 1 for larger effective batch size if memory constrained\n",
    "\n",
    "# --- Saving/Loading Parameters ---\n",
    "KAGGLE_WORKING_DIR = \"/kaggle/working/\"\n",
    "output_dir = os.path.join(KAGGLE_WORKING_DIR, \"jointmatch_output\")\n",
    "checkpoint_dir = os.path.join(output_dir, \"checkpoints\")\n",
    "best_model_path = os.path.join(output_dir, \"best_model.pth\")\n",
    "save_every_epochs = 5 # How often to save periodic checkpoints\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # torch.backends.cudnn.deterministic = True # Can slow down training\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5173e5-rebrand",
   "metadata": {},
   "source": [
    "## 4. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b5353-rebrand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download and Extract Data (if MANUAL_DOWNLOAD is True) ---\n",
    "if MANUAL_DOWNLOAD:\n",
    "    zip_file_path = './tiny-imagenet-200.zip'\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        if not os.path.exists(zip_file_path):\n",
    "            print(\"Downloading Tiny ImageNet...\")\n",
    "            !wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip -O {zip_file_path}\n",
    "            print(\"Download complete.\")\n",
    "        else:\n",
    "            print(f\"Zip file {zip_file_path} already exists.\")\n",
    "\n",
    "        print(\"Extracting Tiny ImageNet...\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall('.')\n",
    "            print(\"Data extracted.\")\n",
    "        except zipfile.BadZipFile:\n",
    "             print(f\"Error: {zip_file_path} is corrupted or not a zip file. Please re-download.\")\n",
    "             raise\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during extraction: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        print(f\"Tiny ImageNet directory '{DATA_DIR}' already exists.\")\n",
    "elif not os.path.exists(DATA_DIR):\n",
    "     raise FileNotFoundError(f\"Kaggle input directory '{DATA_DIR}' not found. Check dataset path.\")\n",
    "else:\n",
    "    print(f\"Using Tiny ImageNet data from Kaggle input: {DATA_DIR}\")\n",
    "\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VAL_DIR = os.path.join(DATA_DIR, 'val')\n",
    "\n",
    "# --- Organize Validation Folder (Important for ImageFolder) --- \n",
    "val_img_dir = os.path.join(VAL_DIR, 'images')\n",
    "annotations_file = os.path.join(VAL_DIR, 'val_annotations.txt')\n",
    "organized_val_dir_exists = False\n",
    "if os.path.isdir(VAL_DIR):\n",
    "    if not os.path.exists(val_img_dir) and not os.path.exists(annotations_file):\n",
    "        subdirs = [d for d in os.listdir(VAL_DIR) if os.path.isdir(os.path.join(VAL_DIR, d))]\n",
    "        if len(subdirs) == NUM_CLASSES:\n",
    "            organized_val_dir_exists = True\n",
    "            print(\"Validation folder appears already organized.\")\n",
    "\n",
    "if not organized_val_dir_exists and os.path.exists(val_img_dir) and os.path.exists(annotations_file):\n",
    "    print(\"Organizing validation folder...\")\n",
    "    try:\n",
    "        val_data = pd.read_csv(annotations_file, sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
    "        for index, row in tqdm(val_data.iterrows(), total=len(val_data), desc=\"Organizing Val\"):\n",
    "            img_class = row['Class']\n",
    "            img_file = row['File']\n",
    "            class_dir = os.path.join(VAL_DIR, img_class)\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            source_path = os.path.join(val_img_dir, img_file)\n",
    "            dest_path = os.path.join(class_dir, img_file)\n",
    "            if os.path.exists(source_path):\n",
    "                shutil.move(source_path, dest_path)\n",
    "        if os.path.exists(val_img_dir) and not os.listdir(val_img_dir):\n",
    "            os.rmdir(val_img_dir)\n",
    "        os.remove(annotations_file)\n",
    "        print(\"Validation folder organized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error organizing validation folder: {e}\")\n",
    "elif not organized_val_dir_exists:\n",
    "    print(\"Warning: Validation folder not organized and source files missing. Cannot organize.\")\n",
    "\n",
    "# --- Define Data Augmentations ---\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "weak_transform = T.Compose([\n",
    "    T.RandomResizedCrop(IMG_SIZE, scale=(0.2, 1.0)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "strong_transform = T.Compose([\n",
    "    T.RandomResizedCrop(IMG_SIZE, scale=(0.2, 1.0)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandAugment(num_ops=2, magnitude=10),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize(IMG_SIZE + 8),\n",
    "    T.CenterCrop(IMG_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# --- Create Datasets ---\n",
    "try:\n",
    "    full_train_dataset = datasets.ImageFolder(TRAIN_DIR)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Training directory '{TRAIN_DIR}' not found.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading training dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Split training data into labeled and unlabeled sets\n",
    "targets = np.array(full_train_dataset.targets)\n",
    "labeled_indices = []\n",
    "unlabeled_indices = []\n",
    "for i in range(NUM_CLASSES):\n",
    "    class_indices = np.where(targets == i)[0]\n",
    "    if len(class_indices) < NUM_LABELED_PER_CLASS:\n",
    "        print(f\"Warning: Class {i} has {len(class_indices)} samples < {NUM_LABELED_PER_CLASS}. Using all as labeled.\")\n",
    "        labeled_indices.extend(class_indices)\n",
    "    else:\n",
    "        np.random.shuffle(class_indices)\n",
    "        labeled_indices.extend(class_indices[:NUM_LABELED_PER_CLASS])\n",
    "        unlabeled_indices.extend(class_indices[NUM_LABELED_PER_CLASS:])\n",
    "print(f\"Total training samples: {len(full_train_dataset)}\")\n",
    "print(f\"Labeled samples: {len(labeled_indices)}\")\n",
    "print(f\"Unlabeled samples: {len(unlabeled_indices)}\")\n",
    "\n",
    "# Custom Dataset Wrappers (already defined in previous attempt, slightly adjusted)\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        return self.transform(x), y\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, subset, transform_weak, transform_strong):\n",
    "        self.subset = subset\n",
    "        self.transform_weak = transform_weak\n",
    "        self.transform_strong = transform_strong\n",
    "    def __getitem__(self, index):\n",
    "        x, _ = self.subset[index]\n",
    "        return self.transform_weak(x), self.transform_strong(x)\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "labeled_subset = Subset(full_train_dataset, labeled_indices)\n",
    "unlabeled_subset = Subset(full_train_dataset, unlabeled_indices)\n",
    "\n",
    "labeled_dataset = LabeledDataset(labeled_subset, weak_transform)\n",
    "unlabeled_dataset = UnlabeledDataset(unlabeled_subset, weak_transform, strong_transform)\n",
    "\n",
    "# Validation Dataset\n",
    "val_loader = None\n",
    "try:\n",
    "    val_dataset = datasets.ImageFolder(VAL_DIR, transform=val_transform)\n",
    "    if len(val_dataset) > 0:\n",
    "        num_workers = 2 # Kaggle default\n",
    "        pin_memory = True if use_cuda else False\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size * 2, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "        print(f\"Validation dataset loaded with {len(val_dataset)} samples.\")\n",
    "    else:\n",
    "        print(\"Warning: Validation dataset is empty.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Validation directory '{VAL_DIR}' not found or not organized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading validation dataset: {e}\")\n",
    "\n",
    "# --- Create Training DataLoaders ---\n",
    "num_workers = 2\n",
    "pin_memory = True if use_cuda else False\n",
    "labeled_loader = DataLoader(labeled_dataset, batch_size=labeled_batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory, drop_last=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=unlabeled_batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory, drop_last=True)\n",
    "\n",
    "# Create iterators\n",
    "labeled_iter = cycle(labeled_loader)\n",
    "unlabeled_iter = cycle(unlabeled_loader)\n",
    "print(\"DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972b502-rebrand",
   "metadata": {},
   "source": [
    "## 5. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be3ef7-rebrand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load two instances of the EfficientNet model\n",
    "MODEL_ARCH = 'efficientnet-b0'\n",
    "print(f\"Loading model architecture: {MODEL_ARCH}\")\n",
    "try:\n",
    "    model_f = EfficientNet.from_pretrained(MODEL_ARCH, num_classes=NUM_CLASSES)\n",
    "    model_g = EfficientNet.from_pretrained(MODEL_ARCH, num_classes=NUM_CLASSES)\n",
    "    print(f\"Loaded pretrained weights for {MODEL_ARCH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading pretrained model: {e}\")\n",
    "    raise\n",
    "\n",
    "model_f = model_f.to(device)\n",
    "model_g = model_g.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63236d97-rebrand",
   "metadata": {},
   "source": [
    "## 6. Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbaf0cf-rebrand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_f = optim.Adam(model_f.parameters(), lr=lr)\n",
    "optimizer_g = optim.Adam(model_g.parameters(), lr=lr)\n",
    "\n",
    "# Loss functions\n",
    "criterion_s = nn.CrossEntropyLoss().to(device) # Supervised loss\n",
    "criterion_u = nn.CrossEntropyLoss(reduction='none').to(device) # Unlabeled loss (manual reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8f8bd-rebrand",
   "metadata": {},
   "source": [
    "## 7. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-eval-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device, desc=\"Evaluating\"):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        for inputs, labels in tqdm(dataloader, desc=desc, leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_samples if total_samples > 0 else 0\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c43716-rebrand",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d02af-rebrand",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting JointMatch Training...\")\n",
    "start_time = time.time()\n",
    "best_val_acc = -1.0\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "# Initialize EMA probabilities (Needs to be persistent across steps)\n",
    "ema_p = torch.ones(NUM_CLASSES).to(device) / NUM_CLASSES\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model_f.train()\n",
    "    model_g.train()\n",
    "\n",
    "    # Track epoch metrics\n",
    "    epoch_loss_s_f, epoch_loss_u_f, epoch_loss_s_g, epoch_loss_u_g = 0.0, 0.0, 0.0, 0.0\n",
    "    total_batches = 0\n",
    "    conf_f_count, conf_g_count, disagree_count = 0, 0, 0\n",
    "    total_unlabeled_processed = 0\n",
    "\n",
    "    # Use tqdm for the main training loop progress\n",
    "    pbar = tqdm(range(steps_per_epoch), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for step in pbar:\n",
    "        # === Get Batches ===\n",
    "        try:\n",
    "            inputs_l, targets_l = next(labeled_iter)\n",
    "            inputs_u_w, inputs_u_s = next(unlabeled_iter)\n",
    "        except StopIteration:\n",
    "            # Should not happen with cycle, but reset just in case\n",
    "            labeled_iter = cycle(labeled_loader)\n",
    "            unlabeled_iter = cycle(unlabeled_loader)\n",
    "            inputs_l, targets_l = next(labeled_iter)\n",
    "            inputs_u_w, inputs_u_s = next(unlabeled_iter)\n",
    "\n",
    "        inputs_l, targets_l = inputs_l.to(device), targets_l.to(device)\n",
    "        inputs_u_w, inputs_u_s = inputs_u_w.to(device), inputs_u_s.to(device)\n",
    "\n",
    "        # === Supervised Loss ===\n",
    "        logits_l_f = model_f(inputs_l)\n",
    "        logits_l_g = model_g(inputs_l)\n",
    "        loss_s_f = criterion_s(logits_l_f, targets_l)\n",
    "        loss_s_g = criterion_s(logits_l_g, targets_l)\n",
    "\n",
    "        # === Unlabeled Loss Calculation ===\n",
    "        with torch.no_grad():\n",
    "            logits_u_w_f = model_f(inputs_u_w)\n",
    "            logits_u_w_g = model_g(inputs_u_w)\n",
    "            probs_u_w_f = F.softmax(logits_u_w_f, dim=1)\n",
    "            probs_u_w_g = F.softmax(logits_u_w_g, dim=1)\n",
    "\n",
    "            # Update EMA\n",
    "            avg_probs_u_w = (probs_u_w_f + probs_u_w_g) / 2.0\n",
    "            batch_p = avg_probs_u_w.mean(dim=0)\n",
    "            ema_p = ema_decay * ema_p + (1 - ema_decay) * batch_p\n",
    "            ema_p = ema_p.detach()\n",
    "\n",
    "            # Adaptive Thresholds\n",
    "            max_ema_p = torch.max(ema_p)\n",
    "            normalized_ema_p = ema_p / (max_ema_p + 1e-8) # Add epsilon\n",
    "            adaptive_thresholds = base_threshold * normalized_ema_p\n",
    "            min_threshold = 1.0 / NUM_CLASSES\n",
    "            adaptive_thresholds = torch.max(adaptive_thresholds, torch.tensor(min_threshold).to(device))\n",
    "\n",
    "            # Pseudo-labels & Masks\n",
    "            max_probs_f, pseudo_labels_hard_f = torch.max(probs_u_w_f, dim=1)\n",
    "            max_probs_g, pseudo_labels_hard_g = torch.max(probs_u_w_g, dim=1)\n",
    "            thresholds_f = adaptive_thresholds.gather(0, pseudo_labels_hard_f)\n",
    "            thresholds_g = adaptive_thresholds.gather(0, pseudo_labels_hard_g)\n",
    "            mask_f = max_probs_f.ge(thresholds_f).float()\n",
    "            mask_g = max_probs_g.ge(thresholds_g).float()\n",
    "\n",
    "            # Disagreement Weights\n",
    "            disagree_mask = (pseudo_labels_hard_f != pseudo_labels_hard_g).float()\n",
    "            sample_weights = disagreement_weight * disagree_mask + (1.0 - disagreement_weight) * (1.0 - disagree_mask)\n",
    "\n",
    "            # Track stats\n",
    "            num_conf_f = mask_f.sum().item()\n",
    "            num_conf_g = mask_g.sum().item()\n",
    "            num_disagree = disagree_mask.sum().item()\n",
    "            conf_f_count += num_conf_f\n",
    "            conf_g_count += num_conf_g\n",
    "            disagree_count += num_disagree\n",
    "            total_unlabeled_processed += inputs_u_w.size(0)\n",
    "\n",
    "        # === Consistency Loss ===\n",
    "        logits_u_s_f = model_f(inputs_u_s)\n",
    "        logits_u_s_g = model_g(inputs_u_s)\n",
    "\n",
    "        loss_u_f_all = criterion_u(logits_u_s_f, pseudo_labels_hard_g)\n",
    "        loss_u_f = (loss_u_f_all * mask_g * sample_weights).sum() / (num_conf_g + 1e-8)\n",
    "\n",
    "        loss_u_g_all = criterion_u(logits_u_s_g, pseudo_labels_hard_f)\n",
    "        loss_u_g = (loss_u_g_all * mask_f * sample_weights).sum() / (num_conf_f + 1e-8)\n",
    "\n",
    "        loss_u_f = torch.nan_to_num(loss_u_f)\n",
    "        loss_u_g = torch.nan_to_num(loss_u_g)\n",
    "\n",
    "        # === Total Loss & Backpropagation ===\n",
    "        total_loss_f = loss_s_f + unlabeled_loss_weight * loss_u_f\n",
    "        total_loss_g = loss_s_g + unlabeled_loss_weight * loss_u_g\n",
    "\n",
    "        # Normalize loss for accumulation (optional, if steps vary)\n",
    "        # total_loss_f = total_loss_f / gradient_accumulation_steps\n",
    "        # total_loss_g = total_loss_g / gradient_accumulation_steps\n",
    "\n",
    "        # Backward pass for F\n",
    "        optimizer_f.zero_grad()\n",
    "        total_loss_f.backward()\n",
    "        # Clip gradients (optional)\n",
    "        # torch.nn.utils.clip_grad_norm_(model_f.parameters(), max_norm=1.0)\n",
    "        optimizer_f.step()\n",
    "\n",
    "        # Backward pass for G\n",
    "        optimizer_g.zero_grad()\n",
    "        total_loss_g.backward()\n",
    "        # Clip gradients (optional)\n",
    "        # torch.nn.utils.clip_grad_norm_(model_g.parameters(), max_norm=1.0)\n",
    "        optimizer_g.step()\n",
    "\n",
    "        # --- Update Epoch Metrics --- \n",
    "        epoch_loss_s_f += loss_s_f.item()\n",
    "        epoch_loss_u_f += loss_u_f.item()\n",
    "        epoch_loss_s_g += loss_s_g.item()\n",
    "        epoch_loss_u_g += loss_u_g.item()\n",
    "        total_batches += 1\n",
    "\n",
    "        # Update progress bar description\n",
    "        avg_loss = (total_loss_f.item() + total_loss_g.item()) / 2.0\n",
    "        pbar.set_postfix(loss=f\"{avg_loss:.4f}\", \n",
    "                         u_f=f\"{loss_u_f.item():.4f}\", u_g=f\"{loss_u_g.item():.4f}\")\n",
    "\n",
    "    # --- End of Epoch --- \n",
    "    avg_epoch_loss_s_f = epoch_loss_s_f / total_batches\n",
    "    avg_epoch_loss_u_f = epoch_loss_u_f / total_batches\n",
    "    avg_epoch_loss_s_g = epoch_loss_s_g / total_batches\n",
    "    avg_epoch_loss_u_g = epoch_loss_u_g / total_batches\n",
    "    avg_epoch_conf_f = conf_f_count / total_unlabeled_processed if total_unlabeled_processed > 0 else 0\n",
    "    avg_epoch_conf_g = conf_g_count / total_unlabeled_processed if total_unlabeled_processed > 0 else 0\n",
    "    avg_epoch_disagree = disagree_count / total_unlabeled_processed if total_unlabeled_processed > 0 else 0\n",
    "\n",
    "    # Evaluate on validation set (if available)\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    if val_loader:\n",
    "        # Evaluate model_f (or could use model_g or an ensemble)\n",
    "        val_loss, val_acc = evaluate(model_f, val_loader, criterion_s, device, desc=\"Validating\")\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "    else:\n",
    "        history['val_loss'].append(None)\n",
    "        history['val_acc'].append(None)\n",
    "\n",
    "    # Optionally evaluate on a subset of training data (labeled part)\n",
    "    # train_loss_eval, train_acc_eval = evaluate(model_f, labeled_loader, criterion_s, device, desc=\"Eval Train (Labeled)\")\n",
    "    # history['train_loss'].append(train_loss_eval)\n",
    "    # history['train_acc'].append(train_acc_eval)\n",
    "    # For simplicity, we'll just log the average training batch losses\n",
    "    history['train_loss'].append((avg_epoch_loss_s_f + avg_epoch_loss_s_g)/2)\n",
    "    history['train_acc'].append(None) # Placeholder if not evaluating train acc separately\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} Summary ({epoch_duration:.2f}s):\")\n",
    "    print(f\"  Avg Train Loss: S_f={avg_epoch_loss_s_f:.4f}, U_f={avg_epoch_loss_u_f:.4f} | S_g={avg_epoch_loss_s_g:.4f}, U_g={avg_epoch_loss_u_g:.4f}\")\n",
    "    print(f\"  Avg Unlabeled Stats: Conf_F={avg_epoch_conf_f:.3f}, Conf_G={avg_epoch_conf_g:.3f}, Disagree={avg_epoch_disagree:.3f}\")\n",
    "    if val_loader:\n",
    "        print(f\"  Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"  Validation: Skipped (no validation data)\")\n",
    "\n",
    "    # --- Checkpointing --- \n",
    "    # Save periodic checkpoint\n",
    "    if (epoch + 1) % save_every_epochs == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_f_state_dict': model_f.state_dict(),\n",
    "            'model_g_state_dict': model_g.state_dict(),\n",
    "            'optimizer_f_state_dict': optimizer_f.state_dict(),\n",
    "            'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
    "            # Note: ema_p is NOT saved here to avoid issues, training resumes with default ema_p\n",
    "            'val_loss': val_loss, # Save last validation loss for info\n",
    "            'val_acc': val_acc\n",
    "        }, checkpoint_path)\n",
    "        print(f\"  Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "    # Save best model based on validation accuracy\n",
    "    if val_loader and val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        # Save model F's state dict as the best model\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_f_state_dict': model_f.state_dict(),\n",
    "            'val_acc': best_val_acc\n",
    "        }, best_model_path)\n",
    "        print(f\"  New best model saved to {best_model_path} (Val Acc: {best_val_acc:.4f})\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "total_training_time = time.time() - start_time\n",
    "print(f\"\\nTraining finished. Total duration: {total_training_time/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe5b5ba-rebrand",
   "metadata": {},
   "source": [
    "## 9. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aeb62f-rebrand",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Final Evaluation --- \")\n",
    "\n",
    "if not val_loader:\n",
    "    print(\"Skipping final evaluation (no validation loader).\")\n",
    "elif not os.path.exists(best_model_path):\n",
    "    print(f\"Best model file not found at {best_model_path}. Evaluating last state of model_f instead.\")\n",
    "    # Optionally evaluate the model state at the end of training\n",
    "    final_loss, final_acc = evaluate(model_f, val_loader, criterion_s, device, desc=\"Final Eval (Last State)\")\n",
    "    print(f\"\\nFinal Results (Last Epoch Model F) - Loss: {final_loss:.4f} Acc: {final_acc:.4f}\")\n",
    "else:\n",
    "    print(f\"Loading best model from: {best_model_path}\")\n",
    "    try:\n",
    "        checkpoint = torch.load(best_model_path, map_location=device)\n",
    "\n",
    "        # Create a new model instance for evaluation\n",
    "        eval_model = EfficientNet.from_pretrained(MODEL_ARCH, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "        # Load the state dict\n",
    "        if 'model_f_state_dict' in checkpoint:\n",
    "            eval_model.load_state_dict(checkpoint['model_f_state_dict'])\n",
    "            print(f\"Loaded best model state dict (Epoch {checkpoint.get('epoch', 'N/A')}, Val Acc: {checkpoint.get('val_acc', 'N/A'):.4f})\")\n",
    "        else:\n",
    "            raise KeyError(\"Checkpoint does not contain 'model_f_state_dict'.\")\n",
    "\n",
    "        # Evaluate the loaded best model\n",
    "        final_loss, final_acc = evaluate(eval_model, val_loader, criterion_s, device, desc=\"Final Eval (Best)\")\n",
    "        print(f\"\\nFinal Results (Best Model) - Loss: {final_loss:.4f} Acc: {final_acc:.4f}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Best model file not found at {best_model_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or evaluating best model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178d66f-rebrand",
   "metadata": {},
   "source": [
    "## 10. Plotting Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe9cd03-rebrand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple plotting using matplotlib if history was collected\n",
    "if history['val_acc'] and any(v is not None for v in history['val_acc']):\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], 'bo-', label='Training Loss (Avg Batch)')\n",
    "    plt.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Skip plotting train_acc if it wasn't evaluated\n",
    "    # plt.plot(epochs, history['train_acc'], 'bo-', label='Training Accuracy') \n",
    "    plt.plot(epochs, history['val_acc'], 'ro-', label='Validation Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(output_dir, \"training_history.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"\\nTraining history plot saved to {plot_path}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nSkipping plotting (no validation data or history).\" )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}