{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPASS on Tiny ImageNet (SimMatch-based)\n",
    "\n",
    "Demonstrates Semi-Supervised Learning using Ensemble Projectors\n",
    "on the Tiny ImageNet dataset.\n",
    "\n",
    "**Changes from previous version:**\n",
    "- Increased Epochs (50 -> 300) for better SSL convergence.\n",
    "- Lowered Learning Rate (0.03 -> 0.02) for stability over more epochs.\n",
    "- Adjusted Contrastive Loss: Compares student(strong_aug) vs teacher(weak_aug).\n",
    "- Lowered Confidence Threshold (0.95 -> 0.90) to potentially use more unlabeled data earlier.\n",
    "- Added Mask Ratio monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, datasets, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration / Hyperparameters\n",
    "# --- Dataset ---\n",
    "DATA_DIR = './tiny-imagenet-200'\n",
    "TINY_IMAGENET_URL = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
    "NUM_CLASSES = 200\n",
    "# --- Training ---\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 300 # <<< INCREASED EPOCHS\n",
    "BATCH_SIZE = 64 # Labeled batch size\n",
    "MU = 7 # Ratio of unlabeled batch size to labeled batch size (unlabeled_bs = MU * BATCH_SIZE)\n",
    "LR = 0.02 # <<< ADJUSTED LEARNING RATE\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "EMA_DECAY = 0.999 # EMA decay factor for teacher model\n",
    "THRESHOLD = 0.90 # <<< ADJUSTED CONFIDENCE THRESHOLD\n",
    "LAMBDA_U = 1.0 # Weight for unsupervised consistency loss\n",
    "LAMBDA_C = 1.0 # Weight for contrastive loss\n",
    "TEMPERATURE = 0.2 # Temperature for contrastive similarity scaling\n",
    "# --- EPASS ---\n",
    "NUM_PROJECTORS = 3 # Number of ensemble projectors (P in the paper)\n",
    "PROJECTION_DIM = 128 # Output dimension of projectors\n",
    "# --- Labeled Data ---\n",
    "LABELED_RATIO = 0.2 # Fraction of training data to use as labeled (e.g., 0.1 for 10%, 0.2 for 20%)\n",
    "\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "print(f\"Labeled Ratio: {LABELED_RATIO}\")\n",
    "print(f\"Labeled samples per class: {int(500 * LABELED_RATIO)}\")\n",
    "print(f\"Unlabeled batch size: {MU * BATCH_SIZE}\")\n",
    "print(f\"Number of projectors: {NUM_PROJECTORS}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Initial LR: {LR}\")\n",
    "print(f\"Confidence Threshold: {THRESHOLD}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Keep deterministic for reproducibility, might be slightly slower\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Download and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def download_and_extract_tiny_imagenet(url, target_dir):\n",
    "    zip_file = os.path.join('.', os.path.basename(url))\n",
    "    if not os.path.exists(target_dir):\n",
    "        print(\"Tiny ImageNet dataset not found. Downloading...\")\n",
    "        try:\n",
    "            # Use quiet flags for less verbose output during download/extract\n",
    "            subprocess.run(['wget', '-q', url], check=True) \n",
    "            print(\"Download complete. Extracting...\")\n",
    "            subprocess.run(['unzip', '-q', zip_file], check=True)\n",
    "            print(f\"Extraction complete. Dataset in {target_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during download/extraction: {e}\")\n",
    "            print(\"Please download and extract Tiny ImageNet manually from the URL:\")\n",
    "            print(url)\n",
    "            print(f\"And place the 'tiny-imagenet-200' folder in the current directory.\")\n",
    "            return False\n",
    "        finally:\n",
    "            if os.path.exists(zip_file):\n",
    "                os.remove(zip_file) # Clean up zip file\n",
    "    else:\n",
    "        print(\"Tiny ImageNet dataset found.\")\n",
    "    return True\n",
    "\n",
    "# Function to create validation folder structure for ImageFolder\n",
    "def create_val_folder_structure(data_dir):\n",
    "    val_dir = os.path.join(data_dir, 'val')\n",
    "    val_img_dir = os.path.join(val_dir, 'images')\n",
    "    val_annotations_file = os.path.join(val_dir, 'val_annotations.txt')\n",
    "\n",
    "    if not os.path.exists(val_annotations_file):\n",
    "        # If images dir also doesn't exist or has subdirs, assume it's already structured\n",
    "        if not os.path.exists(val_img_dir) or len([d for d in os.listdir(val_dir) if os.path.isdir(os.path.join(val_dir, d))]) > 1:\n",
    "             print(\"Validation folder structure potentially already created or source missing.\")\n",
    "             return\n",
    "        else:\n",
    "             print(f\"Validation annotations file not found at {val_annotations_file}. Cannot restructure validation set.\")\n",
    "             return\n",
    "\n",
    "    # Check if restructuring is already done (presence of subdirectories in val_dir other than 'images')\n",
    "    dirs_in_val = [d for d in os.listdir(val_dir) if os.path.isdir(os.path.join(val_dir, d))]\n",
    "    if len(dirs_in_val) > 1: # Already has class folders besides 'images'\n",
    "        print(\"Validation folder structure seems to be already created.\")\n",
    "        return\n",
    "        \n",
    "    # Check if the images dir actually still exists - maybe it was moved before\n",
    "    if not os.path.exists(val_img_dir):\n",
    "        print(\"Validation images folder doesn't exist, structure likely already created.\")\n",
    "        return\n",
    "\n",
    "    print(\"Restructuring validation folder...\")\n",
    "    # Read annotations and create class folders\n",
    "    val_data = {}\n",
    "    with open(val_annotations_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                img_file, class_id = parts[0], parts[1]\n",
    "                if class_id not in val_data:\n",
    "                    val_data[class_id] = []\n",
    "                val_data[class_id].append(img_file)\n",
    "\n",
    "    # Create class directories and move images\n",
    "    for class_id, img_files in tqdm(val_data.items(), desc=\"Moving val images\"):\n",
    "        class_dir = os.path.join(val_dir, class_id)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        for img_file in img_files:\n",
    "            src_path = os.path.join(val_img_dir, img_file)\n",
    "            dest_path = os.path.join(class_dir, img_file)\n",
    "            if os.path.exists(src_path):\n",
    "                shutil.move(src_path, dest_path)\n",
    "\n",
    "    # Remove original images folder if empty\n",
    "    try:\n",
    "      if os.path.exists(val_img_dir) and not os.listdir(val_img_dir):\n",
    "          os.rmdir(val_img_dir)\n",
    "    except OSError as e:\n",
    "         print(f\"Error removing {val_img_dir}: {e}\") \n",
    "    # Keep val_annotations.txt for reference or remove if desired\n",
    "    # os.remove(val_annotations_file)\n",
    "    print(\"Validation folder restructuring complete.\")\n",
    "\n",
    "\n",
    "# --- Download and Prepare ---\n",
    "if download_and_extract_tiny_imagenet(TINY_IMAGENET_URL, DATA_DIR):\n",
    "    create_val_folder_structure(DATA_DIR)\n",
    "    TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "    VAL_DIR = os.path.join(DATA_DIR, 'val') # Now structured correctly\n",
    "    # Check if directories exist after setup\n",
    "    if not os.path.exists(TRAIN_DIR) or not os.path.exists(VAL_DIR):\n",
    "        print(f\"Error: Training ({TRAIN_DIR}) or Validation ({VAL_DIR}) directory not found after setup.\")\n",
    "        exit()\n",
    "else:\n",
    "    print(\"Exiting due to dataset issues.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentations and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image # Import PIL Image\n",
    "\n",
    "# ImageNet statistics\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Weak Augmentation (for supervised loss, teacher pseudo-labels, contrastive keys)\n",
    "transform_weak = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(64, scale=(0.2, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# Strong Augmentation (for unsupervised consistency loss, contrastive queries)\n",
    "# Using RandAugment - parameters might need tuning\n",
    "transform_strong = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(64, scale=(0.2, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=10), # RandAugment\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# Transform for validation (only resize and normalize)\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize(70), # Slightly larger then crop\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "\n",
    "# Custom Dataset wrapper for SSL (applies two transforms)\n",
    "class SSLDataset(Dataset):\n",
    "    def __init__(self, base_dataset, transform_weak, transform_strong):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.transform_weak = transform_weak\n",
    "        self.transform_strong = transform_strong\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "           img, target = self.base_dataset[index]\n",
    "        except Exception as e:\n",
    "            # print(f\"Error loading item {index}: {e}\") # Suppress print for cleaner logs\n",
    "            # Create dummy data (might introduce noise)\n",
    "            img = Image.new('RGB', (64, 64)) # Requires PIL: from PIL import Image\n",
    "            target = 0 # Or a random target\n",
    "        \n",
    "        try:\n",
    "           img_w = self.transform_weak(img)\n",
    "           img_s = self.transform_strong(img)\n",
    "           return img_w, img_s, target\n",
    "        except Exception as transform_e:\n",
    "             # print(f\"Error applying transform to item {index}: {transform_e}\") # Suppress print\n",
    "             # Handle transform error similar to loading error\n",
    "             img_w = torch.zeros((3, 64, 64))\n",
    "             img_s = torch.zeros((3, 64, 64))\n",
    "             target = 0\n",
    "             return img_w, img_s, target \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "# Dataset for labeled data (only weak transform needed for supervised loss)\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, base_dataset, transform_weak):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.transform_weak = transform_weak\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            img, target = self.base_dataset[index]\n",
    "            img_w = self.transform_weak(img)\n",
    "            return img_w, target\n",
    "        except Exception as e:\n",
    "            # print(f\"Error loading/transforming labeled item {index}: {e}\") # Suppress print\n",
    "            # Return dummy data \n",
    "            img_w = torch.zeros((3, 64, 64))\n",
    "            target = 0 \n",
    "            return img_w, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "\n",
    "# --- Create Base Datasets ---\n",
    "# Wrap ImageFolder to handle potential loading errors (e.g., corrupted files)\n",
    "class SafeImageFolder(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            return super().__getitem__(index)\n",
    "        except Exception as e:\n",
    "            # print(f\"Caught error in SafeImageFolder getitem {index}: {e}\") # Suppress print\n",
    "            # Return None and rely on DataLoader's collate_fn to handle this\n",
    "            return None\n",
    "\n",
    "# Collate function to filter out None values returned by SafeImageFolder\n",
    "def safe_collate(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if not batch:\n",
    "         # If the whole batch is problematic, return tensors of zeros\n",
    "         # This requires knowing the expected output structure\n",
    "         # Assuming (img_w, target) or (img_w, img_s, target)\n",
    "         # Let's assume it's for the unlabeled loader format (most complex)\n",
    "         print(\"Warning: Entire batch failed to load. Returning zeros.\") \n",
    "         bs = BATCH_SIZE # Approximate expected batch size\n",
    "         dummy_w = torch.zeros((bs, 3, 64, 64))\n",
    "         dummy_s = torch.zeros((bs, 3, 64, 64))\n",
    "         dummy_t = torch.zeros((bs,), dtype=torch.long)\n",
    "         # Check context (e.g., which loader called this) to adjust structure if needed\n",
    "         # For simplicity, this assumes the SSLDataset structure. May need refinement.\n",
    "         return dummy_w, dummy_s, dummy_t # Adjust based on loader\n",
    "         \n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "base_train_dataset = SafeImageFolder(TRAIN_DIR) # Basic, before SSL transforms\n",
    "val_dataset = SafeImageFolder(VAL_DIR, transform=transform_val)\n",
    "\n",
    "# --- Split Train into Labeled and Unlabeled ---\n",
    "train_indices = list(range(len(base_train_dataset)))\n",
    "train_targets = [s[1] for s in base_train_dataset.samples]\n",
    "\n",
    "# Stratified split\n",
    "labeled_indices, unlabeled_indices = train_test_split(\n",
    "    train_indices,\n",
    "    test_size=1.0 - LABELED_RATIO,\n",
    "    stratify=train_targets,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Total training samples: {len(base_train_dataset)}\")\n",
    "print(f\"Labeled samples: {len(labeled_indices)}\")\n",
    "print(f\"Unlabeled samples: {len(unlabeled_indices)}\")\n",
    "\n",
    "# Verify labeled split distribution (optional)\n",
    "labeled_targets = [train_targets[i] for i in labeled_indices]\n",
    "labeled_counts = Counter(labeled_targets)\n",
    "print(f\"Labeled samples per class (sample): {list(labeled_counts.items())[:5]}\")\n",
    "unlabeled_targets = [train_targets[i] for i in unlabeled_indices]\n",
    "unlabeled_counts = Counter(unlabeled_targets)\n",
    "\n",
    "\n",
    "# --- Create Final SSL Datasets ---\n",
    "labeled_subset = Subset(base_train_dataset, labeled_indices)\n",
    "unlabeled_subset = Subset(base_train_dataset, unlabeled_indices)\n",
    "\n",
    "labeled_train_dataset = LabeledDataset(labeled_subset, transform_weak)\n",
    "unlabeled_train_dataset = SSLDataset(unlabeled_subset, transform_weak, transform_strong)\n",
    "\n",
    "# --- Data Loaders ---\n",
    "num_workers = 2 # Reduce workers if memory issues arise or errors persist\n",
    "\n",
    "labeled_loader = DataLoader(\n",
    "    labeled_train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True, # Drop last incomplete batch\n",
    "    collate_fn=safe_collate # Use safe collate\n",
    ")\n",
    "\n",
    "unlabeled_loader = DataLoader(\n",
    "    unlabeled_train_dataset,\n",
    "    batch_size=BATCH_SIZE * MU,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True, # Crucial for consistent batch sizes\n",
    "    collate_fn=safe_collate # Use safe collate\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=safe_collate # Use safe collate for validation too\n",
    ")\n",
    "\n",
    "# Estimate labeled class distribution prior (for simplified DA)\n",
    "labeled_class_counts = np.array([labeled_counts.get(i, 0) for i in range(NUM_CLASSES)])\n",
    "# Handle cases where a class might have 0 labeled samples (avoid division by zero)\n",
    "total_labeled = labeled_class_counts.sum()\n",
    "if total_labeled > 0:\n",
    "    p_target = torch.tensor(labeled_class_counts / total_labeled, dtype=torch.float).to(DEVICE)\n",
    "else:\n",
    "    # If no labeled samples, use uniform distribution as a fallback\n",
    "    print(\"Warning: No labeled samples found for prior estimation, using uniform.\")\n",
    "    p_target = torch.ones(NUM_CLASSES, dtype=torch.float).to(DEVICE) / NUM_CLASSES\n",
    "    \n",
    "p_target = p_target.detach() # Ensure it's not part of grad computation\n",
    "print(\"Labeled class prior (p_target shape):\", p_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definition (ResNet + EPASS Head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EPASSModel(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, num_projectors=NUM_PROJECTORS, projection_dim=PROJECTION_DIM, pretrained=True):\n",
    "        super(EPASSModel, self).__init__()\n",
    "        # Load ResNet18 - Use pretrained weights\n",
    "        self.encoder = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        num_ftrs = self.encoder.fc.in_features\n",
    "        self.encoder.fc = nn.Identity() # Remove the original classifier\n",
    "\n",
    "        # Classifier Head\n",
    "        self.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "        # EPASS Projector Heads\n",
    "        self.projectors = nn.ModuleList()\n",
    "        for _ in range(num_projectors):\n",
    "            projector = nn.Sequential(\n",
    "                nn.Linear(num_ftrs, num_ftrs),\n",
    "                nn.BatchNorm1d(num_ftrs), # Added BatchNorm for stability\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(num_ftrs, projection_dim)\n",
    "                # No BatchNorm on the final projection layer is common\n",
    "            )\n",
    "            self.projectors.append(projector)\n",
    "\n",
    "        self.num_projectors = num_projectors\n",
    "\n",
    "    def forward(self, x, return_features=False, return_proj_only=False):\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        if return_proj_only:\n",
    "             # Need to handle features directly in this case\n",
    "             # Ensure projectors are applied correctly\n",
    "             projected_features = [proj(features) for proj in self.projectors]\n",
    "             avg_projection = torch.stack(projected_features, dim=0).mean(dim=0)\n",
    "             return avg_projection\n",
    "        \n",
    "        logits = self.fc(features)\n",
    "        \n",
    "        if return_features:\n",
    "            projected_features = [proj(features) for proj in self.projectors]\n",
    "            avg_projection = torch.stack(projected_features, dim=0).mean(dim=0)\n",
    "            return logits, features, avg_projection # Return raw features too if needed elsewhere\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "# Function to update teacher model using EMA\n",
    "@torch.no_grad()\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Use fixed alpha EMA_DECAY - simpler and often effective\n",
    "    alpha = EMA_DECAY\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)\n",
    "\n",
    "# --- Instantiate Student and Teacher Models ---\n",
    "student_model = EPASSModel(num_classes=NUM_CLASSES, num_projectors=NUM_PROJECTORS, projection_dim=PROJECTION_DIM, pretrained=True).to(DEVICE)\n",
    "teacher_model = EPASSModel(num_classes=NUM_CLASSES, num_projectors=NUM_PROJECTORS, projection_dim=PROJECTION_DIM, pretrained=True).to(DEVICE)\n",
    "\n",
    "# Initialize teacher model to be same as student model\n",
    "for param_q, param_k in zip(student_model.parameters(), teacher_model.parameters()):\n",
    "    param_k.data.copy_(param_q.data)\n",
    "    param_k.requires_grad = False # Teacher model doesn't need gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Supervised Loss\n",
    "criterion_s = nn.CrossEntropyLoss()\n",
    "\n",
    "# Unsupervised Loss (Consistency Regularization)\n",
    "criterion_u = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# Contrastive Loss (InfoNCE style, student-strong vs teacher-weak)\n",
    "def contrastive_loss(proj_q_strong, proj_k_weak_detached, temp=TEMPERATURE):\n",
    "    \"\"\"\n",
    "    Calculates InfoNCE-style contrastive loss within the batch.\n",
    "    proj_q_strong: Projections from student (strong aug query) - shape (N, dim)\n",
    "    proj_k_weak_detached: Projections from teacher (weak aug key, detached) - shape (N, dim)\n",
    "    temp: Temperature scaling\n",
    "    \"\"\"\n",
    "    # Normalize projections\n",
    "    proj_q = F.normalize(proj_q_strong, dim=1)\n",
    "    proj_k = F.normalize(proj_k_weak_detached, dim=1) # Already detached\n",
    "\n",
    "    # Cosine similarity: the dot product of normalized vectors is cosine similarity\n",
    "    sim_matrix = torch.mm(proj_q, proj_k.T) # (N, N)\n",
    "    logits = sim_matrix / temp\n",
    "\n",
    "    # Labels: positive pairs are diagonal elements (i-th strong vs i-th weak)\n",
    "    labels = torch.arange(len(proj_q), device=DEVICE) # Ensure labels are on the correct device\n",
    "\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(student_model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Cosine LR Scheduler with updated T_max\n",
    "steps_per_epoch = min(len(labeled_loader), len(unlabeled_loader)) \n",
    "# Handle case where loader might be empty initially due to errors\n",
    "if steps_per_epoch == 0:\n",
    "    print(\"Error: DataLoaders have zero length. Cannot calculate total_steps.\")\n",
    "    # Set a default or raise an error\n",
    "    total_steps = 1 # Avoid division by zero, but training won't proceed correctly\n",
    "else:\n",
    "     total_steps = EPOCHS * steps_per_epoch\n",
    "     \n",
    "print(f\"Scheduler total steps (T_max): {total_steps}\")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=0) \n",
    "\n",
    "# Trackers\n",
    "train_losses_s = []\n",
    "train_losses_u = []\n",
    "train_losses_c = []\n",
    "train_losses_total = []\n",
    "val_accuracies = []\n",
    "mask_ratios = [] # <<< Added tracker for mask usage\n",
    "best_val_acc = 0.0\n",
    "start_epoch = 0 \n",
    "global_step = 0\n",
    "\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start_time = time.time()\n",
    "    student_model.train()\n",
    "    teacher_model.train() # Set teacher to train mode (for BatchNorm, etc.)\n",
    "\n",
    "    running_loss_s = 0.0\n",
    "    running_loss_u = 0.0\n",
    "    running_loss_c = 0.0\n",
    "    running_loss_total = 0.0\n",
    "    running_mask_ratio = 0.0\n",
    "    actual_batches_processed = 0 # Count batches that didn't fail\n",
    "    \n",
    "    # Check steps_per_epoch again in case loaders are empty now\n",
    "    current_steps_per_epoch = min(len(labeled_loader), len(unlabeled_loader))\n",
    "    if current_steps_per_epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loaders are empty, skipping training epoch.\")\n",
    "        continue # Skip to next epoch or validation\n",
    "\n",
    "    labeled_iter = iter(labeled_loader)\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "\n",
    "    pbar = tqdm(range(current_steps_per_epoch), desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\", leave=False)\n",
    "\n",
    "    for i in pbar:\n",
    "        # --- Get Batches --- \n",
    "        # Handle potential StopIteration or None batches from safe_collate\n",
    "        labeled_batch = None\n",
    "        unlabeled_batch = None\n",
    "        try:\n",
    "            labeled_batch = next(labeled_iter)\n",
    "            unlabeled_batch = next(unlabeled_iter)\n",
    "        except StopIteration:\n",
    "             # This shouldn't happen if iterating range(current_steps_per_epoch)\n",
    "             # but as a safeguard:\n",
    "             print(f\"Warning: StopIteration caught unexpectedly at step {i}.\")\n",
    "             break # Exit inner loop for this epoch\n",
    "        \n",
    "        # Check if safe_collate returned None (whole batch failed)\n",
    "        if labeled_batch is None or unlabeled_batch is None:\n",
    "            print(f\"Skipping step {i} due to None batch from loader.\")\n",
    "            continue # Skip this step\n",
    "            \n",
    "        img_w_l, target_l = labeled_batch\n",
    "        img_w_u, img_s_u, _ = unlabeled_batch\n",
    "      \n",
    "        img_w_l, target_l = img_w_l.to(DEVICE), target_l.to(DEVICE)\n",
    "        img_w_u, img_s_u = img_w_u.to(DEVICE), img_s_u.to(DEVICE)\n",
    "\n",
    "        batch_size_l = img_w_l.shape[0]\n",
    "        batch_size_u = img_w_u.shape[0]\n",
    "        \n",
    "        # Skip if batches somehow ended up empty after filtering\n",
    "        if batch_size_l == 0 or batch_size_u == 0:\n",
    "             # print(f\"Skipping step {i} due to zero batch size after filtering.\")\n",
    "             continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # --- Supervised Loss ---\n",
    "        logits_l = student_model(img_w_l)\n",
    "        loss_s = criterion_s(logits_l, target_l)\n",
    "\n",
    "        # --- Unsupervised Loss ---\n",
    "        with torch.no_grad():\n",
    "            logits_u_w = teacher_model(img_w_u)\n",
    "            probs_u_w = torch.softmax(logits_u_w, dim=1)\n",
    "            max_probs, pseudo_labels = torch.max(probs_u_w, dim=1)\n",
    "            mask = max_probs.ge(THRESHOLD).float()\n",
    "            current_mask_ratio = mask.mean().item()\n",
    "\n",
    "        logits_u_s = student_model(img_s_u)\n",
    "        loss_u_unmasked = criterion_u(logits_u_s, pseudo_labels)\n",
    "        # Apply mask and calculate mean only over masked samples\n",
    "        # Multiply by mask, sum it, and divide by the sum of the mask (number of samples above threshold)\n",
    "        loss_u = (loss_u_unmasked * mask).sum() / (mask.sum() + 1e-8) # Add epsilon for stability if mask sum is 0\n",
    "        if mask.sum() == 0: # If no samples passed threshold, loss_u is 0\n",
    "            loss_u = torch.tensor(0.0).to(DEVICE)\n",
    "\n",
    "        # --- Contrastive Loss --- \n",
    "        _ , _, proj_s_s_avg = student_model(img_s_u, return_features=True) \n",
    "        with torch.no_grad():\n",
    "             proj_t_w_avg = teacher_model(img_w_u, return_proj_only=True) \n",
    "        \n",
    "        loss_c = contrastive_loss(proj_s_s_avg, proj_t_w_avg, temp=TEMPERATURE)\n",
    "\n",
    "        # --- Total Loss ---\n",
    "        # Ensure loss_u is a tensor before adding\n",
    "        if not isinstance(loss_u, torch.Tensor):\n",
    "            loss_u = torch.tensor(loss_u, device=DEVICE)\n",
    "        total_loss = loss_s + LAMBDA_U * loss_u + LAMBDA_C * loss_c\n",
    "\n",
    "        # --- Backward and Optimize ---\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step() \n",
    "\n",
    "        # --- Update EMA Teacher ---\n",
    "        global_step += 1\n",
    "        update_ema_variables(student_model, teacher_model, EMA_DECAY, global_step)\n",
    "\n",
    "        # --- Record Losses ---\n",
    "        running_loss_s += loss_s.item()\n",
    "        # Ensure loss_u item is added correctly\n",
    "        running_loss_u += loss_u.item() if isinstance(loss_u, torch.Tensor) else loss_u\n",
    "        running_loss_c += loss_c.item()\n",
    "        running_loss_total += total_loss.item()\n",
    "        running_mask_ratio += current_mask_ratio\n",
    "        actual_batches_processed += 1 # Increment count for successful batches\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{EPOCHS} | Ls: {loss_s.item():.2f} | Lu: {loss_u.item() if isinstance(loss_u, torch.Tensor) else loss_u:.2f} | Lc: {loss_c.item():.2f} | Mask: {current_mask_ratio:.2f}\")\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # --- End of Epoch ---\n",
    "    epoch_time = time.time() - start_time\n",
    "    if actual_batches_processed == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] Time: {epoch_time:.2f}s - No batches processed.\")\n",
    "        # Append placeholder values or skip appending for this epoch\n",
    "        train_losses_s.append(float('nan'))\n",
    "        train_losses_u.append(float('nan'))\n",
    "        train_losses_c.append(float('nan'))\n",
    "        train_losses_total.append(float('nan'))\n",
    "        mask_ratios.append(float('nan')) \n",
    "    else:\n",
    "        avg_loss_s = running_loss_s / actual_batches_processed\n",
    "        avg_loss_u = running_loss_u / actual_batches_processed\n",
    "        avg_loss_c = running_loss_c / actual_batches_processed\n",
    "        avg_loss_total = running_loss_total / actual_batches_processed\n",
    "        avg_mask_ratio = running_mask_ratio / actual_batches_processed \n",
    "        \n",
    "        train_losses_s.append(avg_loss_s)\n",
    "        train_losses_u.append(avg_loss_u)\n",
    "        train_losses_c.append(avg_loss_c)\n",
    "        train_losses_total.append(avg_loss_total)\n",
    "        mask_ratios.append(avg_mask_ratio) \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] Time: {epoch_time:.2f}s\")\n",
    "        print(f\"  Train Loss: Total={avg_loss_total:.4f} (S={avg_loss_s:.4f}, U={avg_loss_u:.4f}, C={avg_loss_c:.4f}) | Mask Ratio: {avg_mask_ratio:.2f}\")\n",
    "        \n",
    "\n",
    "    # --- Validation ---\n",
    "    teacher_model.eval() \n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0.0\n",
    "    actual_val_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            if val_batch is None: \n",
    "                # print(\"Skipping None validation batch\")\n",
    "                continue\n",
    "            images, labels = val_batch\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            if images.nelement() == 0: \n",
    "                continue\n",
    "                \n",
    "            outputs = teacher_model(images)\n",
    "            loss = criterion_s(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            actual_val_batches += 1\n",
    "    \n",
    "    if val_total == 0: \n",
    "        val_accuracy = 0\n",
    "        avg_val_loss = float('inf')\n",
    "        print(\"Warning: Validation set processing resulted in zero valid samples.\")\n",
    "    else:\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        # Average loss over batches that were actually processed\n",
    "        avg_val_loss = val_loss / actual_val_batches if actual_val_batches > 0 else float('inf')\n",
    "        \n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'student_state_dict': student_model.state_dict(),\n",
    "        'teacher_state_dict': teacher_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'train_losses_s': train_losses_s,\n",
    "        'train_losses_u': train_losses_u,\n",
    "        'train_losses_c': train_losses_c,\n",
    "        'train_losses_total': train_losses_total,\n",
    "        'mask_ratios': mask_ratios,\n",
    "        'global_step': global_step\n",
    "    }\n",
    "    torch.save(checkpoint, 'latest_checkpoint.pth')\n",
    "\n",
    "    # Save the best performing model\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        print(f\"  *** New Best Validation Accuracy: {best_val_acc:.2f}% - Saving model... ***\")\n",
    "        torch.save(teacher_model.state_dict(), 'best_teacher_model.pth')\n",
    "\n",
    "print(\"Training Finished!\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization (Losses and Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics(epochs_range, train_losses_s, train_losses_u, train_losses_c, train_losses_total, val_accuracies, mask_ratios, best_acc):\n",
    "    # Filter out potential NaN values if epochs were skipped\n",
    "    valid_indices = [i for i, acc in enumerate(val_accuracies) if not np.isnan(acc)]\n",
    "    if not valid_indices:\n",
    "        print(\"No valid data points to plot.\")\n",
    "        return\n",
    "        \n",
    "    epochs_plot = [epochs_range[i] for i in valid_indices]\n",
    "    train_s_plot = [train_losses_s[i] for i in valid_indices]\n",
    "    train_u_plot = [train_losses_u[i] for i in valid_indices]\n",
    "    train_c_plot = [train_losses_c[i] for i in valid_indices]\n",
    "    train_total_plot = [train_losses_total[i] for i in valid_indices]\n",
    "    val_acc_plot = [val_accuracies[i] for i in valid_indices]\n",
    "    mask_ratios_plot = [mask_ratios[i] for i in valid_indices]\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(22, 6)) # Increased figsize for 3 plots\n",
    "\n",
    "    # Plot Losses\n",
    "    axes[0].plot(epochs_plot, train_s_plot, label='Supervised Loss (Ls)', marker='.', alpha=0.7)\n",
    "    axes[0].plot(epochs_plot, train_u_plot, label='Unsupervised Loss (Lu)', marker='.', alpha=0.7)\n",
    "    axes[0].plot(epochs_plot, train_c_plot, label='Contrastive Loss (Lc)', marker='.', alpha=0.7)\n",
    "    axes[0].plot(epochs_plot, train_total_plot, label='Total Training Loss', marker='o', linewidth=2)\n",
    "    axes[0].set_title('Training Losses per Epoch')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    # Set y-axis limit to zoom in if losses become small\n",
    "    if train_total_plot:\n",
    "        min_loss_display = max(0, min(filter(lambda x: not np.isnan(x), train_total_plot)) - 0.5)\n",
    "        max_loss_display = min(max(filter(lambda x: not np.isnan(x), train_total_plot)) + 0.5, 10) # Cap max display\n",
    "        if max_loss_display > min_loss_display: \n",
    "           axes[0].set_ylim([min_loss_display, max_loss_display])\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    axes[1].plot(epochs_plot, val_acc_plot, label='Validation Accuracy', marker='o', color='crimson')\n",
    "    axes[1].axhline(y=best_acc, color='green', linestyle='--', label=f'Best Accuracy ({best_acc:.2f}%)')\n",
    "    axes[1].set_title('Validation Accuracy per Epoch')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    if val_acc_plot:\n",
    "      min_acc_display = max(0, min(filter(lambda x: not np.isnan(x), val_acc_plot)) - 5) \n",
    "      max_acc_display = min(100, max(filter(lambda x: not np.isnan(x), val_acc_plot)) + 5) \n",
    "      if max_acc_display > min_acc_display:\n",
    "            axes[1].set_ylim([min_acc_display, max_acc_display])\n",
    "            \n",
    "    # Plot Mask Ratio\n",
    "    axes[2].plot(epochs_plot, mask_ratios_plot, label='Mask Ratio (Avg % Used)', marker='x', color='purple')\n",
    "    axes[2].set_title('Avg. Mask Ratio per Epoch')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Ratio')\n",
    "    axes[2].set_ylim([0, 1]) # Ratio is between 0 and 1\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Interpretation for Overfitting/Underfitting:\n",
    "    print(\"\\n--- Overfitting/Underfitting Analysis ---\")\n",
    "    print(\"Observe the plots:\")\n",
    "    print(\"- Underfitting: Training Loss high, Validation Accuracy low/plateaued early.\")\n",
    "    print(\"- Good Fit: Training Loss decreases, Validation Accuracy increases and plateaus high.\")\n",
    "    print(\"- Overfitting: Training Loss decreasing, Validation Accuracy plateaued/decreasing.\")\n",
    "    print(\"- Mask Ratio: Shows the fraction of unlabeled data meeting the confidence threshold. Should ideally increase over time.\")\n",
    "    print(\"-----------------------------------------\")\n",
    "\n",
    "# --- Plot the results ---\n",
    "# Check if tracking lists are populated before plotting\n",
    "if EPOCHS > 0 and val_accuracies:\n",
    "    full_epochs_range = range(1, len(val_accuracies) + 1)\n",
    "    plot_metrics(full_epochs_range, train_losses_s, train_losses_u, train_losses_c, train_losses_total, val_accuracies, mask_ratios, best_val_acc)\n",
    "else:\n",
    "    print(\"Not enough data to plot metrics. Did training run?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}