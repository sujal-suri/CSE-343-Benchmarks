{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78caa5b0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# EPASS with SimMatch Base for Freesound Audio Tagging\n",
    "\n",
    "This notebook implements the **EPASS (Ensemble Projectors Aided for Semi-supervised Learning)** algorithm, using **SimMatch** as the base semi-supervised framework, for audio classification on the Freesound dataset (2018).\n",
    "\n",
    "**Core Concepts:**\n",
    "\n",
    "1.  **SimMatch Base:** Leverages both pseudo-labeling (like FixMatch) and instance similarity matching (contrastive learning) using two strongly augmented views of unlabeled data.\n",
    "2.  **EPASS Enhancement:** Instead of a single MLP projector head (mapping encoder features to embeddings for contrastive loss), EPASS uses *multiple* projector heads. The embeddings from these heads are ensembled (averaged) to produce a more robust and less biased representation.\n",
    "3.  **Goal:** Train models with 20% and 80% labeled data, aiming for high accuracy, demonstrating overfitting/underfitting via plots, and saving the best overall model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f26bc05",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a05f6e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fefae8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Audio & Spectrogram Params\n",
    "        self.sr = 32000           # Audio sample rate\n",
    "        self.duration = 5         # Audio duration (seconds)\n",
    "        self.n_mels = 128         # Number of Mel bands\n",
    "        self.n_fft = 1024         # FFT size\n",
    "        self.hop_length = 512     # Hop length\n",
    "\n",
    "        # Training Params\n",
    "        self.batch_size = 32      # Combined batch size (adjust per GPU memory)\n",
    "        self.epochs = 50          # Number of epochs (adjust as needed for convergence/overfitting demo)\n",
    "        self.lr = 3e-4            # Learning rate (Adam default often works well)\n",
    "        self.num_classes = 41     # Number of classes (as per train.csv)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.seed = 42            # Random seed for reproducibility\n",
    "        self.num_workers = 2      # Dataloader workers\n",
    "\n",
    "        # Semi-Supervised Params (SimMatch + EPASS)\n",
    "        self.labeled_percents = [0.2, 0.8] # Percentages of labeled data to train with [20%, 80%]\n",
    "        self.val_percent = 0.1    # Percentage of *original* training data for validation\n",
    "        self.mu = 7               # Ratio of unlabeled to labeled samples per batch (unlabeled_bs = mu * labeled_bs)\n",
    "        self.wu = 1.0             # Unsupervised classification loss weight\n",
    "        self.wc = 1.0             # Contrastive loss weight (SimMatch component)\n",
    "        self.threshold = 0.95     # Confidence threshold (tau) for pseudo-labeling\n",
    "        self.temperature = 0.1    # Temperature T for contrastive loss (SimMatch component)\n",
    "        self.embedding_dim = 128  # Dimension of the projected embeddings\n",
    "        self.num_projectors = 3   # Number of projectors for EPASS\n",
    "\n",
    "        # SpecAugment Params (for strong augmentation)\n",
    "        self.freq_mask_param = 27\n",
    "        self.time_mask_param = 70 # Adjusted based on spectrogram width\n",
    "        \n",
    "        # Model Saving\n",
    "        self.model_save_path = \"best_epass_simmatch_model.pth\"\n",
    "        \n",
    "        # Data paths (update if necessary)\n",
    "        self.train_csv_path = \"/kaggle/input/freesound-audio-tagging-2018/train.csv\"\n",
    "        self.test_csv_path = \"/kaggle/input/freesound-audio-tagging-2018/test_post_competition.csv\"\n",
    "        self.audio_train_dir = \"/kaggle/input/freesound-audio-tagging-2018/audio_train\"\n",
    "        self.audio_test_dir  = \"/kaggle/input/freesound-audio-tagging-2018/audio_test\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Seed everything for reproducibility\n",
    "random.seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(config.seed)\n",
    "    torch.cuda.manual_seed_all(config.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Number of projectors (EPASS): {config.num_projectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7538d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 2. Audio Preprocessing & Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa98877",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2.1 Audio Preprocessing Function\n",
    "# ----------------------------\n",
    "def preprocess_audio(path, sr=config.sr, duration=config.duration, n_mels=config.n_mels, n_fft=config.n_fft, hop_length=config.hop_length):\n",
    "    try:\n",
    "        y, _ = librosa.load(path, sr=sr)\n",
    "        max_len = sr * duration\n",
    "        # Pad or truncate to fixed length\n",
    "        if len(y) < max_len:\n",
    "            y = np.pad(y, (0, max_len - len(y)))\n",
    "        else:\n",
    "            y = y[:max_len]\n",
    "        # Compute mel spectrogram\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "        # Normalize to [0, 1]\n",
    "        mel_min = mel_db.min()\n",
    "        mel_max = mel_db.max()\n",
    "        if mel_max == mel_min: # Avoid division by zero for silent clips\n",
    "             return np.zeros_like(mel_db, dtype=np.float32)\n",
    "        mel_norm = (mel_db - mel_min) / (mel_max - mel_min)\n",
    "        return mel_norm.astype(np.float32)  # shape: (n_mels, time)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {e}\")\n",
    "        # Return a zero array or handle error appropriately\n",
    "        time_steps = int(max_len / hop_length) + 1 # Approximate time steps\n",
    "        return np.zeros((n_mels, time_steps), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ce1c12",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Augmentation Functions\n",
    "\n",
    "- **Weak Augmentation:** Identity (no change).\n",
    "- **Strong Augmentation:** SpecAugment (frequency and time masking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d0710a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2.2 Augmentation Functions\n",
    "# ----------------------------\n",
    "\n",
    "# Weak augmentation (identity)\n",
    "def weak_augment(mel_spec):\n",
    "    # Ensure input is a tensor and add channel dim\n",
    "    if not isinstance(mel_spec, torch.Tensor):\n",
    "        mel_spec = torch.tensor(mel_spec)\n",
    "    return mel_spec.unsqueeze(0)\n",
    "\n",
    "# Strong augmentation (SpecAugment)\n",
    "spec_augment = torchaudio.transforms.SpecAugment(\n",
    "    freq_masking_param=config.freq_mask_param,\n",
    "    time_masking_param=config.time_mask_param,\n",
    "    # Set masks to 1, as we need two strong views for SimMatch contrastive loss\n",
    "    freq_mask_count=1, \n",
    "    time_mask_count=1, \n",
    "    iid_masks=True\n",
    ")\n",
    "\n",
    "def strong_augment(mel_spec):\n",
    "     # Ensure input is a tensor and add channel dim\n",
    "    if not isinstance(mel_spec, torch.Tensor):\n",
    "        mel_spec = torch.tensor(mel_spec)\n",
    "    mel_tensor = mel_spec.unsqueeze(0) \n",
    "    augmented_mel = spec_augment(mel_tensor)\n",
    "    return augmented_mel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625811c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3. Dataset Classes\n",
    "\n",
    "- Labeled dataset returns one weakly augmented view and the label.\n",
    "- Unlabeled dataset returns one weakly augmented view and *two* differently strongly augmented views (for SimMatch contrastive loss).\n",
    "- Test/Validation dataset returns one weakly augmented view (or none) and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f457f3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. Dataset Classes\n",
    "# ----------------------------\n",
    "\n",
    "# Dataset for Labeled Data\n",
    "class FreesoundLabeledDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, label_map, transform=preprocess_audio, augment=weak_augment):\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.label_map = label_map\n",
    "        self.transform = transform\n",
    "        self.augment = augment # Only weak augmentation needed for supervised loss\n",
    "        self.fnames = df.index.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.fnames[idx]\n",
    "        file_path = os.path.join(self.audio_dir, fname)\n",
    "        mel = self.transform(file_path)\n",
    "        mel_tensor_aug = self.augment(mel) # (1, n_mels, time)\n",
    "        label = self.label_map[self.df.loc[fname, 'label']]\n",
    "        return mel_tensor_aug, torch.tensor(label)\n",
    "\n",
    "# Dataset for Unlabeled Data\n",
    "class FreesoundUnlabeledDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, label_map, transform=preprocess_audio, weak_aug=weak_augment, strong_aug=strong_augment):\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.label_map = label_map # Keep label map for potential analysis, but don't return label\n",
    "        self.transform = transform\n",
    "        self.weak_aug = weak_aug\n",
    "        self.strong_aug = strong_aug\n",
    "        self.fnames = df.index.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.fnames[idx]\n",
    "        file_path = os.path.join(self.audio_dir, fname)\n",
    "        mel = self.transform(file_path)\n",
    "        mel_tensor_weak = self.weak_aug(mel)     # For pseudo-label generation\n",
    "        mel_tensor_strong1 = self.strong_aug(mel) # For classification & contrastive loss\n",
    "        mel_tensor_strong2 = self.strong_aug(mel) # For contrastive loss\n",
    "        # We don't return the true label for unlabeled data during training\n",
    "        return mel_tensor_weak, mel_tensor_strong1, mel_tensor_strong2\n",
    "\n",
    "# Dataset for Testing/Validation (uses weak augmentation/no augmentation)\n",
    "class FreesoundEvalDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, label_map, transform=preprocess_audio, augment=weak_augment):\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.label_map = label_map\n",
    "        self.transform = transform\n",
    "        self.augment = augment # Use weak/no augment for eval consistency\n",
    "        self.fnames = df.index.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.fnames[idx]\n",
    "        file_path = os.path.join(self.audio_dir, fname)\n",
    "        mel = self.transform(file_path)\n",
    "        mel_tensor = self.augment(mel) # (1, n_mels, time)\n",
    "        label = self.label_map[self.df.loc[fname, 'label']]\n",
    "        return mel_tensor, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda90a72",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 4. Prepare Metadata, Label Map, and Data Splits\n",
    "\n",
    "- Load `train.csv` and `test_post_competition.csv`.\n",
    "- Create the label map.\n",
    "- Split the original `train.csv` data into training and validation sets.\n",
    "- Within the training loop, further split the training set into labeled and unlabeled based on the current `labeled_percent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350681f8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4. Prepare Metadata and Label Map\n",
    "# ----------------------------\n",
    "# Load training CSV\n",
    "train_df_full = pd.read_csv(config.train_csv_path)\n",
    "# Ensure fname is index for easy lookup\n",
    "if 'fname' in train_df_full.columns:\n",
    "    train_df_full.set_index(\"fname\", inplace=True)\n",
    "\n",
    "# Load test CSV (for final evaluation - assuming it has labels)\n",
    "try:\n",
    "    test_df = pd.read_csv(config.test_csv_path)\n",
    "    if 'fname' in test_df.columns:\n",
    "        test_df.set_index(\"fname\", inplace=True)\n",
    "    # Ensure test set has labels for evaluation\n",
    "    if 'label' not in test_df.columns or test_df['label'].isnull().any():\n",
    "         print(\"Warning: Test CSV does not contain labels or has missing labels. Using manually_verified column if available.\")\n",
    "         # Try using 'manually_verified' if 'label' is missing/incomplete \n",
    "         if 'manually_verified' in test_df.columns and test_df['manually_verified'].notnull().all():\n",
    "             # Heuristic: Assume verified files are correctly labeled by filename pattern or other logic if needed.\n",
    "             # This part might need competition-specific logic if labels aren't directly provided.\n",
    "             # For now, let's assume the test set *is* labeled for evaluation simplicity.\n",
    "             print(\"Test set seems labeled based on filename/verification. Proceeding with evaluation.\")\n",
    "         else:\n",
    "             print(\"Cannot evaluate on test set without ground truth labels.\")\n",
    "             test_df = None # Disable test evaluation\n",
    "    else:\n",
    "        test_df = test_df.dropna(subset=['label'])\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Test CSV not found at {config.test_csv_path}. Skipping test evaluation.\")\n",
    "    test_df = None\n",
    "\n",
    "# Create label mapping (alphabetical order)\n",
    "labels = sorted(train_df_full['label'].unique())\n",
    "label_map = {label: idx for idx, label in enumerate(labels)}\n",
    "idx_to_label = {idx: label for label, idx in label_map.items()}\n",
    "config.num_classes = len(labels) # Update num_classes based on actual data\n",
    "print(f\"Number of classes: {config.num_classes}\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# --- Split Train/Validation --- \n",
    "# Split the *full* training data first to get a held-out validation set\n",
    "# Use StratifiedShuffleSplit to ensure representative split even if we run only once\n",
    "sss_val = StratifiedShuffleSplit(n_splits=1, test_size=config.val_percent, random_state=config.seed)\n",
    "train_idx, val_idx = next(sss_val.split(train_df_full.index, train_df_full['label']))\n",
    "\n",
    "train_df = train_df_full.iloc[train_idx]\n",
    "val_df = train_df_full.iloc[val_idx]\n",
    "\n",
    "print(f\"\\nFull training samples: {len(train_df_full)}\")\n",
    "print(f\"Split into: Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
    "if test_df is not None:\n",
    "    print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# We will split train_df further into labeled/unlabeled inside the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc93479",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5. Define the Model Architecture (Encoder + Classifier + EPASS Projectors)\n",
    "\n",
    "- Use a pre-trained ResNet18 as the backbone encoder.\n",
    "- Modify the first convolutional layer for 1-channel (spectrogram) input.\n",
    "- Add a single linear classifier head.\n",
    "- Add **multiple** MLP projector heads (EPASS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a1c39",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 5. Define the Model Architecture (Encoder + Classifier + Projectors)\n",
    "# ------------------------------------------------------------------\n",
    "class EpassSimMatchNet(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_dim, num_projectors, pretrained=True):\n",
    "        super().__init__()\n",
    "        # Encoder (ResNet18 base)\n",
    "        base_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "        base_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.encoder = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        encoder_output_dim = base_model.fc.in_features # 512 for ResNet18\n",
    "        \n",
    "        # Classifier Head\n",
    "        self.fc = nn.Linear(encoder_output_dim, num_classes)\n",
    "        \n",
    "        # EPASS Projector Heads (Multiple MLPs)\n",
    "        self.projectors = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(encoder_output_dim, encoder_output_dim), # Optional: intermediate layer\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(encoder_output_dim, embedding_dim)\n",
    "            ) for _ in range(num_projectors)\n",
    "        ])\n",
    "        self.num_projectors = num_projectors\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        flat_features = torch.flatten(features, 1)\n",
    "        \n",
    "        # Classification logits\n",
    "        logits = self.fc(flat_features)\n",
    "        \n",
    "        # Get embeddings from all projectors\n",
    "        embeddings = [proj(flat_features) for proj in self.projectors]\n",
    "        \n",
    "        # Ensemble (average) embeddings for contrastive loss\n",
    "        # Stack along a new dimension (e.g., dim 0), then mean\n",
    "        ensembled_embedding = torch.mean(torch.stack(embeddings, dim=0), dim=0)\n",
    "        \n",
    "        # Return logits for classification and the *ensembled* embedding for contrastive loss\n",
    "        return logits, ensembled_embedding\n",
    "\n",
    "# Instantiate the model\n",
    "model = EpassSimMatchNet(\n",
    "    num_classes=config.num_classes,\n",
    "    embedding_dim=config.embedding_dim,\n",
    "    num_projectors=config.num_projectors\n",
    ").to(config.device)\n",
    "\n",
    "print(f\"Model created with {config.num_projectors} projectors and moved to device.\")\n",
    "# Optional: Print model summary or number of parameters\n",
    "# print(model)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd657344",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 6. Create DataLoaders\n",
    "\n",
    "- Create DataLoaders for labeled, unlabeled (dynamically sized), validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2681a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6. Create DataLoaders (Helper Function)\n",
    "# ----------------------------\n",
    "def create_dataloaders(labeled_df, unlabeled_df, val_df, test_df, config):\n",
    "    train_labeled_dataset = FreesoundLabeledDataset(labeled_df, config.audio_train_dir, label_map)\n",
    "    train_unlabeled_dataset = FreesoundUnlabeledDataset(unlabeled_df, config.audio_train_dir, label_map)\n",
    "    val_dataset = FreesoundEvalDataset(val_df, config.audio_train_dir, label_map)\n",
    "    test_dataset = FreesoundEvalDataset(test_df, config.audio_test_dir, label_map) if test_df is not None else None\n",
    "\n",
    "    # Calculate batch sizes based on mu ratio\n",
    "    # Ensure labeled batch size is at least 1\n",
    "    labeled_bs = max(1, config.batch_size // (config.mu + 1))\n",
    "    unlabeled_bs = config.batch_size - labeled_bs\n",
    "    print(f\"  Using Labeled BS: {labeled_bs}, Unlabeled BS: {unlabeled_bs}\")\n",
    "    \n",
    "    labeled_loader = DataLoader(train_labeled_dataset, \n",
    "                              batch_size=labeled_bs, \n",
    "                              shuffle=True, \n",
    "                              num_workers=config.num_workers, \n",
    "                              drop_last=True) # Drop last if not divisible\n",
    "    \n",
    "    unlabeled_loader = DataLoader(train_unlabeled_dataset, \n",
    "                                batch_size=unlabeled_bs, \n",
    "                                shuffle=True, \n",
    "                                num_workers=config.num_workers, \n",
    "                                drop_last=True) # Drop last if not divisible\n",
    "                                \n",
    "    val_loader = DataLoader(val_dataset, \n",
    "                            batch_size=config.batch_size, # Use full batch size for eval \n",
    "                            shuffle=False, \n",
    "                            num_workers=config.num_workers)\n",
    "                            \n",
    "    test_loader = DataLoader(test_dataset, \n",
    "                             batch_size=config.batch_size, \n",
    "                             shuffle=False, \n",
    "                             num_workers=config.num_workers) if test_dataset is not None else None\n",
    "    \n",
    "    print(f\"  Loaders created. Num labeled batches/epoch: {len(labeled_loader)}, Num unlabeled batches/epoch: {len(unlabeled_loader)}\")\n",
    "    return labeled_loader, unlabeled_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b84478c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 7. Define Training and Evaluation Functions\n",
    "\n",
    "- **`train_one_epoch`**: \n",
    "  - Takes model, optimizer, labeled/unlabeled loaders, loss criteria.\n",
    "  - Iterates through both loaders simultaneously.\n",
    "  - Calculates supervised loss (`loss_s`) on labeled data.\n",
    "  - Calculates unsupervised classification loss (`loss_u`) using pseudo-labels.\n",
    "  - Calculates unsupervised contrastive loss (`loss_c`) using ensembled embeddings from two strong views (SimMatch + EPASS).\n",
    "  - Combines losses and performs backpropagation.\n",
    "- **`evaluate`**: \n",
    "  - Standard evaluation loop using the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7a8dd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# 7. Training and Evaluation Functions\n",
    "# ---------------------------------------\n",
    "\n",
    "def train_one_epoch(model, optimizer, labeled_loader, unlabeled_loader, criterion_s, criterion_u, criterion_c, epoch):\n",
    "    model.train()\n",
    "    running_loss_s = 0.0\n",
    "    running_loss_u = 0.0\n",
    "    running_loss_c = 0.0\n",
    "    correct_labeled = 0\n",
    "    total_labeled = 0\n",
    "    mask_ratios = []\n",
    "\n",
    "    # Ensure unlabeled loader defines the epoch length\n",
    "    num_batches = len(unlabeled_loader) \n",
    "    # Use cycle for the potentially smaller labeled loader\n",
    "    labeled_iter = itertools.cycle(labeled_loader)\n",
    "    \n",
    "    train_iterator = tqdm(unlabeled_loader, total=num_batches, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch_idx, (inputs_u_w, inputs_u_s1, inputs_u_s2) in enumerate(train_iterator):\n",
    "        # Get labeled data for this step\n",
    "        try:\n",
    "            inputs_l, labels_l = next(labeled_iter)\n",
    "        except StopIteration:\n",
    "            # Should not happen if unlabeled_loader is longer and drop_last=True for both\n",
    "            print(\"Warning: Labeled loader exhausted unexpectedly.\")\n",
    "            continue \n",
    "            \n",
    "        # Move data to device\n",
    "        inputs_l, labels_l = inputs_l.to(config.device), labels_l.to(config.device)\n",
    "        inputs_u_w = inputs_u_w.to(config.device)\n",
    "        inputs_u_s1 = inputs_u_s1.to(config.device)\n",
    "        inputs_u_s2 = inputs_u_s2.to(config.device)\n",
    "        \n",
    "        labeled_bs = inputs_l.size(0)\n",
    "        unlabeled_bs = inputs_u_w.size(0)\n",
    "\n",
    "        # --- Supervised Loss --- \n",
    "        logits_l, _ = model(inputs_l) # We only need logits for supervised loss\n",
    "        loss_s = criterion_s(logits_l, labels_l)\n",
    "        \n",
    "        # --- Unsupervised Losses --- \n",
    "        # 1. Pseudo-Labeling Loss (Classification Consistency)\n",
    "        with torch.no_grad():\n",
    "            logits_u_w, _ = model(inputs_u_w)\n",
    "            probs_u_w = torch.softmax(logits_u_w, dim=1)\n",
    "            max_probs, pseudo_labels_u = torch.max(probs_u_w, dim=1)\n",
    "            mask = (max_probs >= config.threshold).float()\n",
    "            mask_ratios.append(mask.mean().item())\n",
    "\n",
    "        logits_u_s1, embeddings_s1 = model(inputs_u_s1)\n",
    "        loss_u_vec = criterion_u(logits_u_s1, pseudo_labels_u)\n",
    "        loss_u = (loss_u_vec * mask).mean() # Apply mask\n",
    "\n",
    "        # 2. Contrastive Loss (Instance Similarity using EPASS embeddings)\n",
    "        _, embeddings_s2 = model(inputs_u_s2) # Only need embeddings for the second strong view\n",
    "        \n",
    "        # Normalize the ensembled embeddings (important for contrastive loss)\n",
    "        embeddings_s1_norm = F.normalize(embeddings_s1, dim=1)\n",
    "        embeddings_s2_norm = F.normalize(embeddings_s2, dim=1)\n",
    "        \n",
    "        # SimMatch Contrastive Loss Calculation (simplified version: compare s1 vs s2)\n",
    "        # Calculate similarity matrix (dot product)\n",
    "        sim_matrix = torch.mm(embeddings_s1_norm, embeddings_s2_norm.t()) / config.temperature\n",
    "        \n",
    "        # Targets: identity matrix (match corresponding augmented views)\n",
    "        targets = torch.arange(unlabeled_bs).to(config.device)\n",
    "        \n",
    "        # Calculate cross-entropy loss (symmetric: compare s1->s2 and s2->s1)\n",
    "        loss_c_vec1 = criterion_c(sim_matrix, targets)\n",
    "        loss_c_vec2 = criterion_c(sim_matrix.t(), targets) # Symmetric loss\n",
    "        loss_c = (loss_c_vec1 + loss_c_vec2) / 2.0\n",
    "        loss_c = (loss_c * mask).mean() # Apply the same mask as pseudo-labeling loss\n",
    "        \n",
    "        # --- Combine Losses --- \n",
    "        total_loss = loss_s + config.wu * loss_u + config.wc * loss_c\n",
    "\n",
    "        # --- Backpropagation and Optimization --- \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- Statistics --- \n",
    "        running_loss_s += loss_s.item() * labeled_bs\n",
    "        running_loss_u += loss_u.item() * unlabeled_bs # Use unlabeled_bs for unsupervised loss avg\n",
    "        running_loss_c += loss_c.item() * unlabeled_bs # Use unlabeled_bs for contrastive loss avg\n",
    "\n",
    "        preds_l = logits_l.argmax(dim=1)\n",
    "        correct_labeled += (preds_l == labels_l).sum().item()\n",
    "        total_labeled += labeled_bs\n",
    "\n",
    "        # Update progress bar\n",
    "        train_iterator.set_postfix(Loss=f\"{total_loss.item():.4f}\", Ls=f\"{loss_s.item():.4f}\", Lu=f\"{loss_u.item():.4f}\", Lc=f\"{loss_c.item():.4f}\", Mask=f\"{np.mean(mask_ratios[-10:]):.2f}\")\n",
    "            \n",
    "    # Calculate average losses and accuracy for the epoch\n",
    "    # Use total labeled samples for Ls and total unlabeled samples processed for Lu, Lc\n",
    "    total_unlabeled_processed = num_batches * unlabeled_loader.batch_size\n",
    "    avg_loss_s = running_loss_s / total_labeled if total_labeled > 0 else 0\n",
    "    avg_loss_u = running_loss_u / total_unlabeled_processed if total_unlabeled_processed > 0 else 0\n",
    "    avg_loss_c = running_loss_c / total_unlabeled_processed if total_unlabeled_processed > 0 else 0\n",
    "    acc_labeled = correct_labeled / total_labeled if total_labeled > 0 else 0\n",
    "    avg_mask_ratio = np.mean(mask_ratios) if mask_ratios else 0\n",
    "\n",
    "    return avg_loss_s, avg_loss_u, avg_loss_c, acc_labeled, avg_mask_ratio\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Only use logits for evaluation\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    epoch_loss = running_loss / total if total > 0 else 0\n",
    "    epoch_acc = correct / total if total > 0 else 0\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f27987",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 8. Training Loop\n",
    "\n",
    "- Initialize model, optimizer, loss functions.\n",
    "- Loop through specified labeled data percentages (20%, 80%).\n",
    "  - For each percentage:\n",
    "    - Split the training data into labeled and unlabeled sets.\n",
    "    - Create dataloaders.\n",
    "    - Re-initialize model weights and optimizer for a fair comparison.\n",
    "    - Loop through epochs:\n",
    "      - Call `train_one_epoch`.\n",
    "      - Call `evaluate` on the validation set.\n",
    "      - Track history (losses, accuracies, mask ratio).\n",
    "      - Check if current validation accuracy is the best *overall*.\n",
    "      - If so, save the model's state dictionary and record the best accuracy and corresponding labeled percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0976a783",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8. Training Loop \n",
    "# ----------------------------\n",
    "criterion_s = nn.CrossEntropyLoss() # Supervised loss\n",
    "criterion_u = nn.CrossEntropyLoss(reduction='none') # Unsupervised classification loss \n",
    "criterion_c = nn.CrossEntropyLoss(reduction='none') # Contrastive loss (applied per-sample, then masked & averaged)\n",
    "\n",
    "best_val_acc_overall = 0.0\n",
    "best_model_state = None\n",
    "best_labeled_percent = -1\n",
    "history = {}\n",
    "\n",
    "for labeled_percent in config.labeled_percents:\n",
    "    print(f\"\\n----- Training with {labeled_percent*100:.0f}% Labeled Data -----\")\n",
    "    history[labeled_percent] = {'train_loss_s': [], 'train_loss_u': [], 'train_loss_c': [], \n",
    "                                'train_acc_l': [], 'val_loss': [], 'val_acc': [], 'mask_ratio': []}\n",
    "\n",
    "    # --- Create Labeled/Unlabeled Split for this run ---\n",
    "    sss_label = StratifiedShuffleSplit(n_splits=1, train_size=labeled_percent, random_state=config.seed + int(labeled_percent*100))\n",
    "    # Use train_df (which excludes validation data)\n",
    "    labeled_idx, unlabeled_idx = next(sss_label.split(train_df.index, train_df['label']))\n",
    "    labeled_df_run = train_df.iloc[labeled_idx]\n",
    "    unlabeled_df_run = train_df.iloc[unlabeled_idx]\n",
    "    print(f\"  Labeled samples for this run: {len(labeled_df_run)}\")\n",
    "    print(f\"  Unlabeled samples for this run: {len(unlabeled_df_run)}\")\n",
    "\n",
    "    # --- Create DataLoaders for this run ---\n",
    "    labeled_loader, unlabeled_loader, val_loader, test_loader = create_dataloaders(\n",
    "        labeled_df_run, unlabeled_df_run, val_df, test_df, config\n",
    "    )\n",
    "\n",
    "    # --- Re-initialize Model and Optimizer for each run ---\n",
    "    print(\"  Re-initializing model and optimizer...\")\n",
    "    model = EpassSimMatchNet(\n",
    "        num_classes=config.num_classes,\n",
    "        embedding_dim=config.embedding_dim,\n",
    "        num_projectors=config.num_projectors\n",
    "    ).to(config.device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    # Optional: Learning rate scheduler\n",
    "    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n",
    "\n",
    "    best_val_acc_run = 0.0 # Best validation accuracy for *this* run\n",
    "\n",
    "    # --- Epoch Loop for this run ---\n",
    "    for epoch in range(config.epochs):\n",
    "        tr_loss_s, tr_loss_u, tr_loss_c, tr_acc_l, mask_ratio = train_one_epoch(\n",
    "            model, optimizer, labeled_loader, unlabeled_loader, \n",
    "            criterion_s, criterion_u, criterion_c, epoch\n",
    "        )\n",
    "        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion_s, config.device)\n",
    "        \n",
    "        # Optional: Step the scheduler\n",
    "        # scheduler.step()\n",
    "\n",
    "        # Log history for this run\n",
    "        history[labeled_percent]['train_loss_s'].append(tr_loss_s)\n",
    "        history[labeled_percent]['train_loss_u'].append(tr_loss_u)\n",
    "        history[labeled_percent]['train_loss_c'].append(tr_loss_c)\n",
    "        history[labeled_percent]['train_acc_l'].append(tr_acc_l)\n",
    "        history[labeled_percent]['val_loss'].append(val_loss)\n",
    "        history[labeled_percent]['val_acc'].append(val_acc)\n",
    "        history[labeled_percent]['mask_ratio'].append(mask_ratio)\n",
    "\n",
    "        print(f\"  Epoch {epoch+1}/{config.epochs} -> \"\n",
    "              f\"Loss S: {tr_loss_s:.4f}, Loss U: {tr_loss_u:.4f}, Loss C: {tr_loss_c:.4f}, Acc (L): {tr_acc_l:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | Mask Ratio: {mask_ratio:.3f}\")\n",
    "\n",
    "        # Check if this is the best model *overall*\n",
    "        if val_acc > best_val_acc_overall:\n",
    "            best_val_acc_overall = val_acc\n",
    "            best_labeled_percent = labeled_percent\n",
    "            best_model_state = copy.deepcopy(model.state_dict()) # Deep copy state dict\n",
    "            print(f\"  *** New best validation accuracy overall: {best_val_acc_overall:.4f} (from {labeled_percent*100:.0f}% run). Saving model state... ***\")\n",
    "            # Save immediately or just store the state dict and save at the end\n",
    "            # torch.save(best_model_state, config.model_save_path)\n",
    "            \n",
    "# --- End of Training Loop --- \n",
    "print(f\"\\nFinished training across all label percentages.\")\n",
    "print(f\"Best overall validation accuracy: {best_val_acc_overall:.4f} achieved with {best_labeled_percent*100:.0f}% labeled data.\")\n",
    "\n",
    "# Save the overall best model state if found\n",
    "if best_model_state is not None:\n",
    "    print(f\"Saving the best overall model state to {config.model_save_path}\")\n",
    "    torch.save(best_model_state, config.model_save_path)\n",
    "else:\n",
    "    print(\"No best model state was saved (perhaps validation accuracy never improved?).\")\n",
    "\n",
    "# Load the best model for final evaluation\n",
    "print(f\"\\nLoading best overall model for final evaluation...\")\n",
    "if best_model_state is not None:\n",
    "    model = EpassSimMatchNet( # Recreate the model structure\n",
    "        num_classes=config.num_classes,\n",
    "        embedding_dim=config.embedding_dim,\n",
    "        num_projectors=config.num_projectors\n",
    "    ).to(config.device)\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Best model loaded successfully.\")\n",
    "else:\n",
    "    print(\"Could not load a best model state. Evaluation will use the model from the last epoch of the last run.\")\n",
    "    # 'model' variable still holds the last trained model \n",
    "\n",
    "# Ensure test_loader was created if test_df exists\n",
    "if test_df is not None:\n",
    "     # Need to create test_loader if it wasn't created in the last loop iteration\n",
    "     # (This assumes val_df exists from the initial split)\n",
    "     _, _, _, test_loader = create_dataloaders(labeled_df_run, unlabeled_df_run, val_df, test_df, config)\n",
    "else:\n",
    "     test_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74ac75",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 9. Evaluate on Test Set and Compute Metrics\n",
    "\n",
    "- Use the loaded best-performing model state.\n",
    "- Run `evaluate` on the `test_loader` (if available).\n",
    "- Print final metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f14c3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 9. Evaluate on Test Set and Compute Metrics\n",
    "# ------------------------------------------\n",
    "\n",
    "if test_loader is not None and best_model_state is not None:\n",
    "    print(\"\\nEvaluating the best model on the Test Set...\")\n",
    "    test_loss, test_acc, y_pred_test, y_true_test = evaluate(model, test_loader, criterion_s, config.device)\n",
    "    print(f\"\\nFinal Test Results using Best Overall Model (Val Acc: {best_val_acc_overall:.4f}, Labeled: {best_labeled_percent*100:.0f}%):\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report on Test Set:\")\n",
    "    # Use idx_to_label to get class names\n",
    "    target_names = [idx_to_label[i] for i in range(config.num_classes)]\n",
    "    print(classification_report(y_true_test, y_pred_test, target_names=target_names, digits=4))\n",
    "\n",
    "    print(\"\\nConfusion Matrix on Test Set:\")\n",
    "    cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=False, fmt='d', xticklabels=target_names, yticklabels=target_names, cmap='Blues') # Annot=False for large matrices\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix on Test Data (Best EPASS+SimMatch Model)\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif test_loader is None:\n",
    "    print(\"\\nSkipping final test evaluation as test data/labels were not available.\")\n",
    "else: # best_model_state is None\n",
    "    print(\"\\nSkipping final test evaluation as no best model was saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b29fca",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 10. Plot Training Curves\n",
    "\n",
    "- Plot losses (Supervised, Unsupervised Classification, Contrastive) and accuracies (Train Labeled, Validation) for **each** labeled percentage run to show overfitting/underfitting trends under different supervision levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4799d533",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 10. Plot Training Curves\n",
    "# ----------------------------\n",
    "num_runs = len(config.labeled_percents)\n",
    "fig, axes = plt.subplots(num_runs, 3, figsize=(18, 6 * num_runs), squeeze=False)\n",
    "\n",
    "for i, percent in enumerate(config.labeled_percents):\n",
    "    run_history = history[percent]\n",
    "    epochs_range = range(1, len(run_history['train_loss_s']) + 1)\n",
    "    \n",
    "    # Plot Losses\n",
    "    ax = axes[i, 0]\n",
    "    ax.plot(epochs_range, run_history['train_loss_s'], label='Train Loss S')\n",
    "    ax.plot(epochs_range, run_history['train_loss_u'], label='Train Loss U (Class.)')\n",
    "    ax.plot(epochs_range, run_history['train_loss_c'], label='Train Loss C (Contrast.)')\n",
    "    ax.plot(epochs_range, run_history['val_loss'], label='Val Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(f'Loss Curves ({percent*100:.0f}% Labeled)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Plot Accuracies\n",
    "    ax = axes[i, 1]\n",
    "    ax.plot(epochs_range, run_history['train_acc_l'], label='Train Acc (on Labeled)')\n",
    "    ax.plot(epochs_range, run_history['val_acc'], label='Val Acc')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Accuracy Curves ({percent*100:.0f}% Labeled)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    ax.axhline(y=best_val_acc_overall if best_labeled_percent == percent else 0, color='r', linestyle='--', label=f'Best Overall Val Acc ({best_val_acc_overall:.3f})' if best_labeled_percent == percent else None)\n",
    "    if best_labeled_percent == percent: ax.legend() # Show legend only if this run was best\n",
    "\n",
    "    # Plot Mask Ratio\n",
    "    ax = axes[i, 2]\n",
    "    ax.plot(epochs_range, run_history['mask_ratio'], label='Pseudo-Label Mask Ratio')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Ratio')\n",
    "    ax.set_title(f'Mask Ratio Curve ({percent*100:.0f}% Labeled)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.suptitle('EPASS + SimMatch Training Progress', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Analysis of Curves ---\")\n",
    "print(\"Overfitting: Indicated if validation accuracy plateaus/decreases while training accuracy continues to rise, or if validation loss increases while training loss decreases.\")\n",
    "print(\"Underfitting: Indicated if both training and validation accuracies are low and plateau early, or if losses remain high.\")\n",
    "print(f\"Target Accuracy (~80%): Observe if the best validation accuracy ({best_val_acc_overall:.4f}) reached the target.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 862232,
     "sourceId": 8900,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 206951,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-24T10:43:08.968551",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}